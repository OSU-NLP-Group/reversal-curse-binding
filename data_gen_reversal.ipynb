{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from utils import choose, rand_matching, kagebunshin, random_pairing\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "num_GA_entities = 50000           # number of group A entities\n",
    "\n",
    "# multiplicity\n",
    "comb = None     # abstract concept level\n",
    "# comb = 10      # attaching surface-form names\n",
    "\n",
    "num_GB_entities = int(num_GA_entities * 0.6)\n",
    "GA_entities = [\"<s_{}>\".format(i) for i in range(num_GA_entities)]\n",
    "GB_entities = [\"<t_{}>\".format(i) for i in range(num_GB_entities)]\n",
    "all_entities = GA_entities + GB_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_dict(l, num_token=1, comb=None):\n",
    "    \"\"\"\n",
    "    l: a list of distinct entity names\n",
    "    num_token: number of tokens for each entity\n",
    "    comb: multiplicity of f/l names (if not None)\n",
    "\n",
    "    returns: a dict mapping each entitiy in l to its token list\n",
    "    \"\"\"\n",
    "    if num_token == 1:\n",
    "        # identity map\n",
    "        assert comb is None\n",
    "        return {e: [e] for e in l}\n",
    "    \n",
    "    assert not (comb is None)\n",
    "    # add multiplicity for first two tokens, then fill the tails with unique tokens\n",
    "    assert num_token >= 2\n",
    "    num_tail_tokens = num_token - 2\n",
    "    assert type(comb) == int and len(l) % comb == 0\n",
    "    token_dict = dict()\n",
    "    \n",
    "    first, last = [\"<f_{}>\".format(i) for i in range(len(l)//comb)], [\"<l_{}>\".format(i) for i in range(len(l)//comb)]\n",
    "    first, last = kagebunshin(first, comb), kagebunshin(last, comb)\n",
    "    if comb == 1:\n",
    "        tokens = [(first[jj], last[jj]) for jj in range(len(first))]\n",
    "    else:\n",
    "        tokens = rand_matching(first, last)\n",
    "    assert len(tokens) == len(l)\n",
    "\n",
    "    for j in range(len(l)):\n",
    "        e = l[j]\n",
    "        # sanity check\n",
    "        assert e.count(\"<\") == e.count(\">\") == 1\n",
    "        assert e[0] == \"<\" and e[-1] == \">\"\n",
    "        tail = [\"<{}_{}>\".format(e.strip(\"><\"), i) for i in range(num_tail_tokens)]\n",
    "        token_dict[e] = list(tokens[j]) + tail\n",
    "    return token_dict\n",
    "    \n",
    "def form_items(arr, ty, entity_token_dict, vocab):\n",
    "    h,r,t = arr\n",
    "\n",
    "    item_list = []\n",
    "    input_text = [[vocab[tok] for tok in entity_token_dict[h]], [vocab[r]], [vocab[\"<mask>\"]]]\n",
    "    target_text = [[vocab[tok] for tok in entity_token_dict[t]]]\n",
    "    \n",
    "    item_list.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text,\n",
    "        \"type\": ty,\n",
    "    })\n",
    "    return item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comb is None:\n",
    "    ent_token_dict = build_token_dict(all_entities)\n",
    "else:\n",
    "    assert type(comb) == int\n",
    "    ent_token_dict = build_token_dict(all_entities, num_token=2, comb=comb)\n",
    "\n",
    "ent_token_set = set()\n",
    "for v in ent_token_dict.values():\n",
    "    ent_token_set |= set(v)\n",
    "\n",
    "vocab = dict()\n",
    "for tok in list(ent_token_set) + [\"<mask>\"]:\n",
    "    assert tok not in vocab\n",
    "    vocab[tok] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rel_pairs = 6\n",
    "train, atomic, test = [], [], []\n",
    "\n",
    "for i in tqdm(range(num_rel_pairs)):\n",
    "    rel, rel_inv = \"<r_{}>\".format(i), \"<r_{}_inv>\".format(i)     # add _inv if want r != r^{-1}\n",
    "    for tok in [rel, rel_inv]:\n",
    "        assert tok not in vocab\n",
    "        vocab[tok] = len(vocab)\n",
    "\n",
    "    for (ent1, ent2) in random_pairing(GA_entities):\n",
    "        # add both directions into training set for learning the rules\n",
    "        train += form_items([ent1, rel, ent2], 'train', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "        train += form_items([ent2, rel_inv, ent1], 'train', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "\n",
    "    for (ent1, ent2) in random_pairing(GB_entities):\n",
    "        # add one direction into training set, and the other into test set\n",
    "        if random.uniform(0,1) <= 0.5:\n",
    "            atomic += form_items([ent1, rel, ent2], 'atomic', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "            test += form_items([ent2, rel_inv, ent1], 'test', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "        else:\n",
    "            atomic += form_items([ent2, rel_inv, ent1], 'atomic', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "            test += form_items([ent1, rel, ent2], 'test', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "\n",
    "print(\"train/atomic/test:\", len(train), len(atomic), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad token at the end\n",
    "for tok in [\"<pad>\"]:\n",
    "    assert tok not in vocab\n",
    "    vocab[tok] = len(vocab)\n",
    "\n",
    "assert len(vocab) == len(set(vocab.keys())) == len(set(vocab.values()))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 3000\n",
    "\n",
    "if comb is None:\n",
    "    dataset = \"inversionid.{}.{}\".format(num_GA_entities, num_GB_entities)\n",
    "else:\n",
    "    dataset = \"inversionidcomb{}.{}.{}\".format(comb, num_GA_entities, num_GB_entities)\n",
    "\n",
    "os.makedirs(\"data/{}\".format(dataset), exist_ok=True)\n",
    "\n",
    "with open(\"data/{}/train.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(train + atomic, f)\n",
    "\n",
    "probes = {\n",
    "    \"train\": choose(train, test_size),\n",
    "    \"atomic\": choose(atomic, test_size),\n",
    "    \"test\": choose(test, test_size),\n",
    "}\n",
    "\n",
    "with open(\"data/{}/valid.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(probes, f)\n",
    "with open(\"data/{}/vocab.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
