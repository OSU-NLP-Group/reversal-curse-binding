{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import choose, rand_matching, kagebunshin, split\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 10007                      # prime in modular addition: all values are from 0 ~ P-1 and mod P\n",
    "core_v_multiplicity = 10\n",
    "num_core_entities = P * core_v_multiplicity\n",
    "\n",
    "all_values = [\"<{}>\".format(i) for i in range(P)]\n",
    "core_entities = [\"<c_{}>\".format(i) for i in range(num_core_entities)]\n",
    "# assign values to core entities\n",
    "temp = all_values * core_v_multiplicity\n",
    "core_entity2values = dict()\n",
    "for i in range(len(core_entities)):\n",
    "    core_entity2values[core_entities[i]] = temp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_dict(l, num_token=1, comb=None):\n",
    "    \"\"\"\n",
    "    l: a list of distinct entity names\n",
    "    num_token: number of tokens for each entity\n",
    "    comb: multiplicity of f/l names (if not None)\n",
    "\n",
    "    returns: a dict mapping each entitiy in l to its token list\n",
    "    \"\"\"\n",
    "    if num_token == 1:\n",
    "        # identity map\n",
    "        assert comb is None\n",
    "        return {e: [e] for e in l}\n",
    "    \n",
    "    assert not (comb is None)\n",
    "    # add multiplicity for first two tokens, then fill the tails with unique tokens\n",
    "    # comb: number of distinct first/second tokens\n",
    "    assert num_token >= 2\n",
    "    num_tail_tokens = num_token - 2\n",
    "    assert type(comb) == int\n",
    "    token_dict = dict()\n",
    "\n",
    "    tmp = math.ceil(len(l)/comb)\n",
    "    \n",
    "    first, last = [\"<f_{}>\".format(i) for i in range(tmp)], [\"<l_{}>\".format(i) for i in range(tmp)]\n",
    "    first, last = kagebunshin(first, comb), kagebunshin(last, comb)\n",
    "    if comb == 1:\n",
    "        tokens = [(first[jj], last[jj]) for jj in range(len(first))]\n",
    "    else:\n",
    "        tokens = rand_matching(first, last)\n",
    "    tokens = tokens[:len(l)]\n",
    "\n",
    "    for j in range(len(l)):\n",
    "        e = l[j]\n",
    "        # sanity check\n",
    "        assert e.count(\"<\") == e.count(\">\") == 1\n",
    "        assert e[0] == \"<\" and e[-1] == \">\"\n",
    "        tail = [\"<{}_{}>\".format(e.strip(\"><\"), i) for i in range(num_tail_tokens)]\n",
    "        token_dict[e] = list(tokens[j]) + tail\n",
    "    return token_dict\n",
    "\n",
    "def mod_add(a_, b_, P_=P):\n",
    "    if type(a_) == type(b_) == int:\n",
    "        c = a_ + b_\n",
    "        if c < 0:\n",
    "            c += P_\n",
    "        elif c >= P_:\n",
    "            c -= P_\n",
    "        assert 0 <= c <= P_-1\n",
    "        return c\n",
    "\n",
    "    assert a_.startswith('<') and a_.endswith(\">\") and b_.startswith('<') and b_.endswith(\">\")\n",
    "    a, b = int(a_[1:-1]), int(b_[1:-1])\n",
    "    assert 0 <= a <= P_-1 and 0 <= b <= P_-1\n",
    "    c = a + b\n",
    "    if c < 0:\n",
    "        c += P_\n",
    "    elif c >= P_:\n",
    "        c -= P_\n",
    "    assert 0 <= c <= P_-1\n",
    "    return '<'+str(c)+'>'\n",
    "\n",
    "def mod_subtract(a_, b_, P_=P):\n",
    "    if type(a_) == type(b_) == int:\n",
    "        c = a_ - b_\n",
    "        if c < 0:\n",
    "            c += P_\n",
    "        elif c >= P_:\n",
    "            c -= P_\n",
    "        assert 0 <= c <= P_-1\n",
    "        return c\n",
    "\n",
    "    assert a_.startswith('<') and a_.endswith(\">\") and b_.startswith('<') and b_.endswith(\">\")\n",
    "    a, b = int(a_[1:-1]), int(b_[1:-1])\n",
    "    assert 0 <= a <= P_-1 and 0 <= b <= P_-1\n",
    "    c = a - b\n",
    "    if c < 0:\n",
    "        c += P_\n",
    "    elif c >= P_:\n",
    "        c -= P_\n",
    "    assert 0 <= c <= P_-1\n",
    "    return '<'+str(c)+'>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondarg_values_numerical = choose(list(range(10, 50)), 2)   # choose two small values (excluding 0)\n",
    "X_value_lower_bound, X_value_upper_bound = 200, 800\n",
    "\n",
    "values_banned_source = set(secondarg_values_numerical)   \n",
    "max_ = 4\n",
    "for _ in range(max_):\n",
    "    for v1 in deepcopy(values_banned_source):\n",
    "        for v2 in secondarg_values_numerical:\n",
    "            values_banned_source.add(mod_subtract(v1, v2))\n",
    "\n",
    "values_banned_target = deepcopy(values_banned_source)\n",
    "for _ in range(max_):\n",
    "    for v1 in deepcopy(values_banned_target):\n",
    "        for v2 in secondarg_values_numerical:\n",
    "            values_banned_target.add(mod_add(v1, v2))\n",
    "            values_banned_target.add(mod_subtract(v1, v2))\n",
    "print(secondarg_values_numerical, len(values_banned_source), len(values_banned_target))\n",
    "values_available_target = list(set(range(P)) - values_banned_target)\n",
    "\n",
    "values_available_target = [i for i in values_available_target if X_value_lower_bound<=i<=X_value_upper_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branching_factor = 10\n",
    "num_X_entities = 20           # number of problem instances\n",
    "X_entities = [\"<x_{}>\".format(i) for i in range(num_X_entities)]\n",
    "# assign random distinct values for testing. small ones to make calculations easy\n",
    "X_values = choose(values_available_target, num_X_entities)\n",
    "X_values = [f'<{i}>' for i in X_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bridge1_entities = branching_factor * num_X_entities\n",
    "bridge1_entities = [\"<br1_{}>\".format(i) for i in range(num_bridge1_entities)]\n",
    "\n",
    "num_bridge2_entities = branching_factor**2 * num_X_entities\n",
    "bridge2_entities = [\"<br2_{}>\".format(i) for i in range(num_bridge2_entities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = core_entities + bridge1_entities + bridge2_entities + X_entities\n",
    "\n",
    "comb = None\n",
    "ent_token_dict = build_token_dict(all_entities)\n",
    "# comb = 10\n",
    "# ent_token_dict = build_token_dict(all_entities, num_token=2, comb=comb)\n",
    "\n",
    "ent_token_set = []\n",
    "for v in ent_token_dict.values():\n",
    "    ent_token_set.extend(v)\n",
    "\n",
    "if len(ent_token_set) != len(set(ent_token_set)):\n",
    "    ent_token_set = list(set(ent_token_set))\n",
    "\n",
    "vocab = dict()    # map token to id\n",
    "for tok in all_values + ent_token_set + [\"<=>\", \"<pad>\"]:  # numbers always come first.       \n",
    "    assert tok not in vocab\n",
    "    vocab[tok] = len(vocab)\n",
    "id2token = {val:key for key, val in vocab.items()}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_attr_item(arr, ty, entity_token_dict, vocab):\n",
    "    entity, value = arr\n",
    "    input_text = [[vocab[tok] for tok in entity_token_dict[entity]], [vocab['<=>']]]\n",
    "    target_text = [[vocab[value]]]\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text,\n",
    "        \"type\": ty,\n",
    "    }\n",
    "    return item\n",
    "\n",
    "def form_rel_item(arr, ty, entity_token_dict, vocab):\n",
    "    e1, e2, e3 = arr\n",
    "    if np.random.uniform() < 0.5:\n",
    "        input_text = [[vocab[tok] for tok in entity_token_dict[e1]], [vocab[tok] for tok in entity_token_dict[e2]]]\n",
    "    else:\n",
    "        input_text = [[vocab[tok] for tok in entity_token_dict[e2]], [vocab[tok] for tok in entity_token_dict[e1]]]\n",
    "    target_text = [[vocab[tok] for tok in entity_token_dict[e3]]]\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text,\n",
    "        \"type\": ty,\n",
    "    }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_attr = []       # test\n",
    "for ent, val in zip(X_entities, X_values):\n",
    "    X_attr.append(form_attr_item([ent, val], 'X_attr', entity_token_dict=ent_token_dict, vocab=vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each numerical value or its string version to a list of core entities with that value \n",
    "v2group = dict()\n",
    "for v_added in range(P):\n",
    "    group = []\n",
    "    for k in range(core_v_multiplicity):\n",
    "        group.append(core_entities[k*P + v_added])\n",
    "        assert core_entity2values[group[-1]] == f'<{v_added}>'\n",
    "    v2group[v_added] = group\n",
    "    v2group[f'<{v_added}>'] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data for teaching the rules\n",
    "downsampling_ratio = 0.3\n",
    "core_rel_full = []\n",
    "# rel facts between ID core entities (through available 2nd arg values)\n",
    "for i in tqdm(range(P)):\n",
    "    if i in values_banned_source:\n",
    "        continue\n",
    "    for v_added in secondarg_values_numerical:\n",
    "        j = mod_add(i, v_added)\n",
    "        group1, group2, group3 = v2group[i], v2group[v_added], v2group[j]\n",
    "        for e1 in group1:\n",
    "            for e3 in group3:\n",
    "                # randomly pick a group2 ent.\n",
    "                e2 = choose(group2, 1)[0]\n",
    "                core_rel_full.append(form_rel_item([e1, e2, e3], 'core_rel', entity_token_dict=ent_token_dict, vocab=vocab))\n",
    "print(len(core_rel_full))\n",
    "core_rel, core_rel_test = split(core_rel_full, downsampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_attr = []\n",
    "for ent in core_entities:\n",
    "    core_attr.append(form_attr_item([ent, core_entity2values[ent]], 'core_attr', entity_token_dict=ent_token_dict, vocab=vocab))\n",
    "len(core_rel), len(core_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob = 0.6     # chance of poisoning\n",
    "drop_backward_prob = 0.5\n",
    "num_core_connect_to_leaf = 6\n",
    "\n",
    "br1_attr = []\n",
    "br2_attr = []\n",
    "\n",
    "edge2fact = [defaultdict(set) for _ in range(len(X_entities))]\n",
    "\n",
    "pointer_br1, pointer_br2 = 0, 0\n",
    "for iii in range(len(X_entities)):\n",
    "\n",
    "    core_ents_used = set()\n",
    "    all_values_used = set()\n",
    "\n",
    "    x_ent, x_val = X_entities[iii], X_values[iii]\n",
    "    all_values_used.add(vocab[x_val])\n",
    "\n",
    "    nodes_dropall = choose(list(range(branching_factor)), round(drop_prob * branching_factor))\n",
    "    for node_id in range(branching_factor):\n",
    "        # grab a new bridge1 entity\n",
    "        e_br1 = bridge1_entities[pointer_br1]\n",
    "        # decide whether to nullify all children of the node\n",
    "        doomify = node_id in nodes_dropall\n",
    "        # randomly decide the 2nd arg and the entity from available ones\n",
    "        v_added = id2token[choose(secondarg_values_numerical, 1)[0]]\n",
    "        e2 = choose(v2group[v_added], 1)[0]\n",
    "        ent_pair = (x_ent, e_br1)\n",
    "        if np.random.uniform() < 0.5:\n",
    "            v_br1 = mod_subtract(x_val, v_added)\n",
    "            br1_attr.append(form_attr_item([e_br1, v_br1], 'br1_attr', entity_token_dict=ent_token_dict, vocab=vocab))\n",
    "            edge2fact[iii][ent_pair].add((e_br1, e2, x_ent))\n",
    "        else:\n",
    "            v_br1 = mod_add(x_val, v_added)\n",
    "            br1_attr.append(form_attr_item([e_br1, v_br1], 'br1_attr', entity_token_dict=ent_token_dict, vocab=vocab))\n",
    "            edge2fact[iii][ent_pair].add((x_ent, e2, e_br1))\n",
    "        core_ents_used |= {e2}\n",
    "        all_values_used.add(vocab[v_br1])\n",
    "        assert vocab[v_br1] not in values_banned_source\n",
    "\n",
    "        nodes_dropped_2 = choose(list(range(branching_factor)), round(drop_prob * branching_factor))\n",
    "        for node_id_2 in range(branching_factor):\n",
    "            # whether nullify this node\n",
    "            nullify = doomify or (node_id_2 in nodes_dropped_2)\n",
    "            # cut backward or forward (if nullify)\n",
    "            if np.random.uniform() < drop_backward_prob:\n",
    "                drop_backward = True\n",
    "            else:\n",
    "                drop_backward = False\n",
    "\n",
    "            e_br2 = bridge2_entities[pointer_br2]\n",
    "            v_added = id2token[choose(secondarg_values_numerical, 1)[0]]\n",
    "            e2 = choose(v2group[v_added], 1)[0]\n",
    "            ent_pair = (e_br1, e_br2)\n",
    "            if np.random.uniform() < 0.5:\n",
    "                v_br2 = mod_subtract(v_br1, v_added)\n",
    "                br2_attr.append(form_attr_item([e_br2, v_br2], 'br2_attr', entity_token_dict=ent_token_dict, vocab=vocab))\n",
    "                if nullify and drop_backward:\n",
    "                    pass\n",
    "                else:\n",
    "                    edge2fact[iii][ent_pair].add((e_br2, e2, e_br1))\n",
    "            else:\n",
    "                v_br2 = mod_add(v_br1, v_added)\n",
    "                br2_attr.append(form_attr_item([e_br2, v_br2], 'br2_attr', entity_token_dict=ent_token_dict, vocab=vocab))\n",
    "                if nullify and drop_backward:\n",
    "                    pass\n",
    "                else:\n",
    "                    edge2fact[iii][ent_pair].add((e_br1, e2, e_br2))\n",
    "            core_ents_used |= {e2}\n",
    "            all_values_used.add(vocab[v_br2])\n",
    "            assert vocab[v_br2] not in values_banned_source\n",
    "\n",
    "            if nullify and not drop_backward:\n",
    "                # drop forward: no connection to nodes with known values\n",
    "                pass\n",
    "            else:\n",
    "                # link to known entities. constant branching factor here.\n",
    "                for _ in range(num_core_connect_to_leaf):\n",
    "                    # randomly decide the value to take and randomly assign the source entity from core entities\n",
    "                    v_added = id2token[choose(secondarg_values_numerical, 1)[0]]\n",
    "                    e2 = choose(v2group[v_added], 1)[0]\n",
    "                    if np.random.uniform() < 0.5:\n",
    "                        v_taken = mod_subtract(v_br2, v_added)\n",
    "                        e1 = choose(v2group[v_taken], 1)[0]\n",
    "                        edge2fact[iii][(e_br2, e1)].add((e1, e2, e_br2))\n",
    "                    else:\n",
    "                        v_taken = mod_add(v_br2, v_added)\n",
    "                        e1 = choose(v2group[v_taken], 1)[0]\n",
    "                        edge2fact[iii][(e_br2, e1)].add((e_br2, e2, e1))\n",
    "                    core_ents_used |= {e1, e2}\n",
    "                    all_values_used.add(vocab[v_taken])\n",
    "                    assert vocab[v_taken] not in values_banned_source\n",
    "\n",
    "            pointer_br2 += 1\n",
    "        pointer_br1 += 1\n",
    "    print(f\"{len(core_ents_used)} core entities are used. {len(all_values_used)} distinct values for nodes; max: {max(all_values_used)}, min: {min(all_values_used)}\")\n",
    "\n",
    "assert pointer_br1 == num_bridge1_entities and pointer_br2 == num_bridge2_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# dump the facts for synthesizing data for LLM testing \n",
    "# \"\"\"\n",
    "# all_facts = dict()\n",
    "# for iii in range(len(edge2fact)):\n",
    "#     all_facts[iii] = set()\n",
    "#     count = 0\n",
    "#     for _, val in edge2fact[iii].items():\n",
    "#         count += len(val)\n",
    "#         for temp in val:\n",
    "#             all_facts[iii].add(temp)\n",
    "#     assert count == len(all_facts[iii])\n",
    "#     all_facts[iii] = list(all_facts[iii])\n",
    "# with open(f\"saved_files/all_facts_{branching_factor}.json\", \"w\", encoding='utf-8') as f:\n",
    "#     json.dump(all_facts, f)\n",
    "# with open(f\"saved_files/X_values_{branching_factor}.json\", \"w\", encoding='utf-8') as f:\n",
    "#     json.dump(X_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rel = []      # in train\n",
    "for d in edge2fact:\n",
    "    for _, val in d.items():\n",
    "        for (e1, e2, e3) in val:\n",
    "            X_rel.append(form_rel_item([e1, e2, e3], 'X_rel', entity_token_dict=ent_token_dict, vocab=vocab))\n",
    "print(len(core_rel), len(core_attr), len(X_rel), len(br1_attr), len(br2_attr), len(X_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 512 * 19\n",
    "\n",
    "if comb is None:\n",
    "    dataset = f\"gsmfinal.{P}.{branching_factor}.{drop_prob}\"\n",
    "else:\n",
    "    dataset = f\"gsmfinalcomb{comb}.{P}.{branching_factor}.{drop_prob}\"\n",
    "os.makedirs(\"data/{}\".format(dataset), exist_ok=True)\n",
    "\n",
    "with open(\"data/{}/train.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(core_rel + core_attr + X_rel, f)\n",
    "\n",
    "probes = {\n",
    "    \"core_rel\": choose(core_rel, test_size),\n",
    "    \"core_rel_test\": choose(core_rel_test, test_size),\n",
    "    \"core_attr\": choose(core_attr, test_size),\n",
    "    \"br1_attr\": choose(br1_attr, test_size),\n",
    "    \"br2_attr\": choose(br2_attr, test_size),\n",
    "    \"X_rel\": choose(X_rel, test_size),\n",
    "    \"X_attr\": choose(X_attr, test_size),\n",
    "}\n",
    "\n",
    "with open(\"data/{}/valid.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(probes, f)\n",
    "with open(\"data/{}/vocab.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
